{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "acting-bruce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../pipelines/google_results_count/extract.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '../pipelines/google_results_count/extract.py'\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "def get_search_urls(keyword_list, url=\"https://www.google.com/search?q=\"):\n",
    "    \"\"\" Compose search urls \"\"\"\n",
    "    search_query = [kw.replace(' ','+') for kw in keyword_list] # replace space with '+'\n",
    "    return [url+sq for sq in search_query]\n",
    "    \n",
    "def get_results_count(keyword, user_agent):\n",
    "    result = requests.get(keyword, headers=user_agent)    \n",
    "    soup = BeautifulSoup(result.content, 'html.parser')\n",
    "    \n",
    "    #  string that contains results count 'About 1,410,000,000 results'\n",
    "    total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) \n",
    "    \n",
    "    # extract number\n",
    "    results_num = int(''.join([num for num in total_results_text if num.isdigit()]) )\n",
    "    \n",
    "    return results_num\n",
    "\n",
    "def assert_df(df, keyword_list, url=\"https://www.google.com/search?q=\"):\n",
    "    # create dummy dataframe for comparison\n",
    "    df_compare = pd.DataFrame({\n",
    "        'keyword': pd.Series([*keyword_list], dtype='object'),\n",
    "        'results_count': pd.Series([1 for i in keyword_list], dtype='int64'),\n",
    "        'search_url': pd.Series(get_search_urls(keyword_list, url=url), dtype='object'),\n",
    "        'query_timestamp': pd.Series([datetime.now() for i in keyword_list], dtype='datetime64[ns]')\n",
    "    })\n",
    "\n",
    "    # columns\n",
    "    column_difference = set(df.columns).symmetric_difference(df_compare.columns)\n",
    "    assert len(column_difference) == 0, f\"The following columns differ to reference dataframe: {column_difference}\"\n",
    "    # dtypes\n",
    "    assert (df_compare.dtypes == df.dtypes).all(), f\"Different dtypes for {df.dtypes}\\n{df_compare.dtypes}\"\n",
    "    # length\n",
    "    assert len(df) == len(keyword_list), f\"{len(df)} does not equal {len(keyword_list)}\"\n",
    "    \n",
    "    print(\"Success >>>>>>>>>>\\tDataframe meets expectations\\n\")\n",
    "    \n",
    "def df_build_results_count(keyword_list, user_agent, url=\"https://www.google.com/search?q=\"):\n",
    "    search_urls = get_search_urls(keyword_list)\n",
    "    result_count = [get_results_count(url, user_agent) for url in search_urls]  \n",
    "    timestamp = datetime.now()\n",
    "    \n",
    "    df = pd.DataFrame({'keyword': keyword_list, \n",
    "                       'results_count': result_count, \n",
    "                       'search_url': search_urls, \n",
    "                       'query_timestamp': timestamp})\n",
    "    # testing\n",
    "    assert_df(df=df, keyword_list=keyword_list, url=url)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-convertible",
   "metadata": {},
   "source": [
    "## Test pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "subjective-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- load into csv\n",
    "import pandas as pd\n",
    "\n",
    "def write_to_csv(df, filepath):\n",
    "    print('_'*42, f'\\nExport data, dimension: {df.shape} to\\t{filepath}\\n')\n",
    "    print(df.head(2).to_markdown())\n",
    "    df.to_csv(f'{filepath}', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "green-namibia",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project dir\tC:/Users/phili/Google Drive/Projekter/Google keywords/google_results_count\n",
      "Keywords\t['ESG', 'sustainable finance', 'responsible investment', 'impact investing', 'green finance', 'sustainable investment', 'socially responsible investment']\n",
      "Export\t\tC:/Users/phili/Google Drive/Projekter/Google keywords/google_results_count\\data/0_raw\\google_results_count_20210307_1915.csv\n",
      "Success >>>>>>>>>>\tDataframe meets expectations\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "write_to_csv() got an unexpected keyword argument 'file_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-db89e8ab51f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m                       url=GOOGLE_URL)\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mwrite_to_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFILEPATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: write_to_csv() got an unexpected keyword argument 'file_path'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "# load settings.yml\n",
    "with open(r'../settings.yml') as file:\n",
    "    # The FullLoader parameter handles the conversion from YAML\n",
    "    # scalar values to Python the dictionary format\n",
    "    settings = yaml.full_load(file)\n",
    "\n",
    "    PROJECT_DIR  = settings['project']['root_dir']\n",
    "    RAW_DATA_DIR = settings['project']['raw_data_dir']\n",
    "    FILENAME     = f\"{settings['project']['export_filename']}_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\" \n",
    "    FILEPATH     = os.path.join(PROJECT_DIR, RAW_DATA_DIR, FILENAME)\n",
    "    KEYWORDS     = settings['query']['keywords']\n",
    "    USER_AGENT   = settings['query']['user_agent']\n",
    "    GOOGLE_URL   = settings['query']['google_url']\n",
    "    \n",
    "\n",
    "print(\"Project dir\\t{}\\nKeywords\\t{}\\nExport\\t\\t{}\".format(PROJECT_DIR, KEYWORDS, FILEPATH))\n",
    "\n",
    "df = df_build_results_count(keyword_list=KEYWORDS, \n",
    "                      user_agent=USER_AGENT, \n",
    "                      url=GOOGLE_URL)\n",
    "\n",
    "write_to_csv(df, filepath=FILEPATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
